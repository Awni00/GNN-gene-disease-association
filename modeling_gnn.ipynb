{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load edge list\n",
    "edge_list_path = 'data/edge_list.npy'\n",
    "edge_list = torch.Tensor(np.load(edge_list_path).T).type(torch.int64) # read in format expected by pytorch geometric [2, n_edges]\n",
    "\n",
    "# load protein-ID dictionary (need new ID system starting at index 0 for pytorch geometric)\n",
    "protein_id_dict = np.load('data/protein_ids_dict.npy', allow_pickle=True).item() # maps my custom ID system to Ensembl IDs\n",
    "protein_id_dict_inv = {Ensembl: id_ for id_, Ensembl in protein_id_dict.items()} # maps Ensembl IDs to my custom ID system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/HPAnode_PPInetwork_labels_v2.csv' #NOTE: labels need to be generated from infomation in this dataset (FIXME)\n",
    "node_dataset = pd.read_csv(data_path, index_col=0)\n",
    "\n",
    "# map dataset\n",
    "myID = node_dataset.index.map(protein_id_dict_inv).rename('myID')\n",
    "node_dataset.insert(loc=0, column='myID', value=myID)\n",
    "node_dataset = node_dataset.reset_index().set_index('myID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure dataset with myID is of correct order and format\n",
    "node_dataset.sort_index(inplace=True) # should already be sorted, but just in case\n",
    "assert((node_dataset.index.to_numpy() == np.arange(len(node_dataset))).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1268\n",
       "1    1268\n",
       "Name: my_label, dtype: Int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create positives\n",
    "label_name = 'my_label'\n",
    "\n",
    "# find positives\n",
    "pos_label_col = 'Any_pos' #FIXME: figure out meaning of columns and determing appropriate choice of positive labels\n",
    "pos_labels = pd.array([1 if row[pos_label_col] else None for id_, row in node_dataset.iterrows()], dtype='Int32')\n",
    "node_dataset[label_name] = pos_labels\n",
    "\n",
    "# create negatives\n",
    "def sample_negatives(PU_labels):\n",
    "    '''randomly samples from the unlabeled samples'''\n",
    "\n",
    "    # sample same # as positives\n",
    "    num_pos = (PU_labels==1).sum()\n",
    "    neg_inds = PU_labels[PU_labels.isna()].sample(num_pos).index\n",
    "\n",
    "    # TODO: more sophisticated methods for sampling methods. (e.g.: use mutation rate, unsupervised learning, etc.)\n",
    "\n",
    "    return neg_inds # returns ID's of negative samples\n",
    "\n",
    "neg_label_inds = sample_negatives(node_dataset[label_name])\n",
    "node_dataset[label_name].loc[neg_label_inds] = 0\n",
    "\n",
    "# TODO: save this data for reproducibility (not now, but once this is finalized and fixed)\n",
    "\n",
    "node_dataset[label_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tissue RNA - lung [NX]</th>\n",
       "      <th>Single Cell Type RNA - Mucus-secreting cells [NX]</th>\n",
       "      <th>node_0</th>\n",
       "      <th>node_1</th>\n",
       "      <th>node_2</th>\n",
       "      <th>node_3</th>\n",
       "      <th>node_4</th>\n",
       "      <th>node_5</th>\n",
       "      <th>node_6</th>\n",
       "      <th>node_7</th>\n",
       "      <th>...</th>\n",
       "      <th>node_91</th>\n",
       "      <th>node_92</th>\n",
       "      <th>node_93</th>\n",
       "      <th>node_94</th>\n",
       "      <th>node_95</th>\n",
       "      <th>node_96</th>\n",
       "      <th>node_97</th>\n",
       "      <th>node_98</th>\n",
       "      <th>node_99</th>\n",
       "      <th>my_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>myID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.293745</td>\n",
       "      <td>-0.037880</td>\n",
       "      <td>1.127839</td>\n",
       "      <td>0.280114</td>\n",
       "      <td>-0.562910</td>\n",
       "      <td>0.680988</td>\n",
       "      <td>-0.107089</td>\n",
       "      <td>0.072526</td>\n",
       "      <td>0.313232</td>\n",
       "      <td>0.584403</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024280</td>\n",
       "      <td>-0.001387</td>\n",
       "      <td>0.020212</td>\n",
       "      <td>0.003985</td>\n",
       "      <td>0.038392</td>\n",
       "      <td>-0.000400</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.026843</td>\n",
       "      <td>-0.021281</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.113110</td>\n",
       "      <td>-0.085092</td>\n",
       "      <td>0.917932</td>\n",
       "      <td>0.107147</td>\n",
       "      <td>-0.434965</td>\n",
       "      <td>-0.383316</td>\n",
       "      <td>0.318524</td>\n",
       "      <td>-0.244370</td>\n",
       "      <td>-0.418846</td>\n",
       "      <td>0.215204</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064491</td>\n",
       "      <td>-0.112350</td>\n",
       "      <td>0.170609</td>\n",
       "      <td>-0.053770</td>\n",
       "      <td>0.045259</td>\n",
       "      <td>0.019909</td>\n",
       "      <td>-0.247172</td>\n",
       "      <td>-0.091207</td>\n",
       "      <td>0.132493</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.154398</td>\n",
       "      <td>-0.105079</td>\n",
       "      <td>0.988793</td>\n",
       "      <td>0.956698</td>\n",
       "      <td>1.227737</td>\n",
       "      <td>-0.472868</td>\n",
       "      <td>-0.674905</td>\n",
       "      <td>1.143741</td>\n",
       "      <td>-0.281922</td>\n",
       "      <td>-0.545074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002763</td>\n",
       "      <td>0.020379</td>\n",
       "      <td>-0.005631</td>\n",
       "      <td>-0.036373</td>\n",
       "      <td>0.017688</td>\n",
       "      <td>0.059181</td>\n",
       "      <td>0.044308</td>\n",
       "      <td>-0.040594</td>\n",
       "      <td>-0.096454</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.427931</td>\n",
       "      <td>-0.086981</td>\n",
       "      <td>2.043737</td>\n",
       "      <td>1.748582</td>\n",
       "      <td>1.079505</td>\n",
       "      <td>0.433287</td>\n",
       "      <td>-0.009460</td>\n",
       "      <td>0.671909</td>\n",
       "      <td>-0.519080</td>\n",
       "      <td>-0.146116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124833</td>\n",
       "      <td>0.008721</td>\n",
       "      <td>0.032194</td>\n",
       "      <td>-0.150306</td>\n",
       "      <td>0.071645</td>\n",
       "      <td>-0.257054</td>\n",
       "      <td>0.229697</td>\n",
       "      <td>0.140799</td>\n",
       "      <td>-0.118054</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.175042</td>\n",
       "      <td>-0.068096</td>\n",
       "      <td>1.354122</td>\n",
       "      <td>0.340012</td>\n",
       "      <td>-0.965181</td>\n",
       "      <td>-0.526789</td>\n",
       "      <td>0.439390</td>\n",
       "      <td>0.140608</td>\n",
       "      <td>0.423648</td>\n",
       "      <td>0.819132</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010304</td>\n",
       "      <td>0.042092</td>\n",
       "      <td>0.023966</td>\n",
       "      <td>0.015752</td>\n",
       "      <td>-0.029377</td>\n",
       "      <td>-0.084104</td>\n",
       "      <td>0.018157</td>\n",
       "      <td>-0.005635</td>\n",
       "      <td>0.040090</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14531</th>\n",
       "      <td>-0.567278</td>\n",
       "      <td>-0.102876</td>\n",
       "      <td>1.015411</td>\n",
       "      <td>-0.072371</td>\n",
       "      <td>0.365560</td>\n",
       "      <td>-0.138961</td>\n",
       "      <td>-0.347684</td>\n",
       "      <td>-0.435399</td>\n",
       "      <td>-0.253599</td>\n",
       "      <td>0.219127</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032206</td>\n",
       "      <td>0.024851</td>\n",
       "      <td>-0.013315</td>\n",
       "      <td>-0.006128</td>\n",
       "      <td>-0.015564</td>\n",
       "      <td>0.020981</td>\n",
       "      <td>-0.027659</td>\n",
       "      <td>-0.013440</td>\n",
       "      <td>-0.017793</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14532</th>\n",
       "      <td>-0.495024</td>\n",
       "      <td>-0.100515</td>\n",
       "      <td>1.191883</td>\n",
       "      <td>0.193004</td>\n",
       "      <td>0.741723</td>\n",
       "      <td>-0.290044</td>\n",
       "      <td>-0.536447</td>\n",
       "      <td>-0.683263</td>\n",
       "      <td>-0.090337</td>\n",
       "      <td>0.166989</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033623</td>\n",
       "      <td>0.028087</td>\n",
       "      <td>-0.021190</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>-0.017235</td>\n",
       "      <td>0.022398</td>\n",
       "      <td>-0.042681</td>\n",
       "      <td>-0.006727</td>\n",
       "      <td>-0.020374</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14541</th>\n",
       "      <td>-0.546634</td>\n",
       "      <td>-0.105079</td>\n",
       "      <td>1.425105</td>\n",
       "      <td>-0.314999</td>\n",
       "      <td>-0.784846</td>\n",
       "      <td>0.288170</td>\n",
       "      <td>-0.228677</td>\n",
       "      <td>-0.068500</td>\n",
       "      <td>0.441017</td>\n",
       "      <td>-0.260978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004942</td>\n",
       "      <td>-0.000126</td>\n",
       "      <td>0.029034</td>\n",
       "      <td>-0.011315</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>-0.009829</td>\n",
       "      <td>-0.005649</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>-0.005044</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14548</th>\n",
       "      <td>-0.603405</td>\n",
       "      <td>0.835389</td>\n",
       "      <td>2.009425</td>\n",
       "      <td>1.142514</td>\n",
       "      <td>0.587500</td>\n",
       "      <td>-1.108236</td>\n",
       "      <td>-0.523388</td>\n",
       "      <td>0.751758</td>\n",
       "      <td>0.338092</td>\n",
       "      <td>0.156393</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067621</td>\n",
       "      <td>0.055089</td>\n",
       "      <td>-0.035550</td>\n",
       "      <td>-0.016057</td>\n",
       "      <td>0.030357</td>\n",
       "      <td>0.086635</td>\n",
       "      <td>-0.116914</td>\n",
       "      <td>-0.061450</td>\n",
       "      <td>-0.064007</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14550</th>\n",
       "      <td>-0.422770</td>\n",
       "      <td>-0.092489</td>\n",
       "      <td>0.942309</td>\n",
       "      <td>-0.071680</td>\n",
       "      <td>-0.648776</td>\n",
       "      <td>0.007953</td>\n",
       "      <td>-0.479971</td>\n",
       "      <td>0.004475</td>\n",
       "      <td>0.273699</td>\n",
       "      <td>0.507728</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001730</td>\n",
       "      <td>-0.000357</td>\n",
       "      <td>0.013551</td>\n",
       "      <td>-0.001779</td>\n",
       "      <td>-0.008815</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.003282</td>\n",
       "      <td>-0.002532</td>\n",
       "      <td>-0.006610</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2536 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Tissue RNA - lung [NX]  \\\n",
       "myID                            \n",
       "0                   -0.293745   \n",
       "2                   -0.113110   \n",
       "5                   -0.154398   \n",
       "13                  -0.427931   \n",
       "14                  -0.175042   \n",
       "...                       ...   \n",
       "14531               -0.567278   \n",
       "14532               -0.495024   \n",
       "14541               -0.546634   \n",
       "14548               -0.603405   \n",
       "14550               -0.422770   \n",
       "\n",
       "       Single Cell Type RNA - Mucus-secreting cells [NX]    node_0    node_1  \\\n",
       "myID                                                                           \n",
       "0                                              -0.037880  1.127839  0.280114   \n",
       "2                                              -0.085092  0.917932  0.107147   \n",
       "5                                              -0.105079  0.988793  0.956698   \n",
       "13                                             -0.086981  2.043737  1.748582   \n",
       "14                                             -0.068096  1.354122  0.340012   \n",
       "...                                                  ...       ...       ...   \n",
       "14531                                          -0.102876  1.015411 -0.072371   \n",
       "14532                                          -0.100515  1.191883  0.193004   \n",
       "14541                                          -0.105079  1.425105 -0.314999   \n",
       "14548                                           0.835389  2.009425  1.142514   \n",
       "14550                                          -0.092489  0.942309 -0.071680   \n",
       "\n",
       "         node_2    node_3    node_4    node_5    node_6    node_7  ...  \\\n",
       "myID                                                               ...   \n",
       "0     -0.562910  0.680988 -0.107089  0.072526  0.313232  0.584403  ...   \n",
       "2     -0.434965 -0.383316  0.318524 -0.244370 -0.418846  0.215204  ...   \n",
       "5      1.227737 -0.472868 -0.674905  1.143741 -0.281922 -0.545074  ...   \n",
       "13     1.079505  0.433287 -0.009460  0.671909 -0.519080 -0.146116  ...   \n",
       "14    -0.965181 -0.526789  0.439390  0.140608  0.423648  0.819132  ...   \n",
       "...         ...       ...       ...       ...       ...       ...  ...   \n",
       "14531  0.365560 -0.138961 -0.347684 -0.435399 -0.253599  0.219127  ...   \n",
       "14532  0.741723 -0.290044 -0.536447 -0.683263 -0.090337  0.166989  ...   \n",
       "14541 -0.784846  0.288170 -0.228677 -0.068500  0.441017 -0.260978  ...   \n",
       "14548  0.587500 -1.108236 -0.523388  0.751758  0.338092  0.156393  ...   \n",
       "14550 -0.648776  0.007953 -0.479971  0.004475  0.273699  0.507728  ...   \n",
       "\n",
       "        node_91   node_92   node_93   node_94   node_95   node_96   node_97  \\\n",
       "myID                                                                          \n",
       "0     -0.024280 -0.001387  0.020212  0.003985  0.038392 -0.000400  0.005882   \n",
       "2     -0.064491 -0.112350  0.170609 -0.053770  0.045259  0.019909 -0.247172   \n",
       "5      0.002763  0.020379 -0.005631 -0.036373  0.017688  0.059181  0.044308   \n",
       "13     0.124833  0.008721  0.032194 -0.150306  0.071645 -0.257054  0.229697   \n",
       "14    -0.010304  0.042092  0.023966  0.015752 -0.029377 -0.084104  0.018157   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "14531 -0.032206  0.024851 -0.013315 -0.006128 -0.015564  0.020981 -0.027659   \n",
       "14532 -0.033623  0.028087 -0.021190  0.000559 -0.017235  0.022398 -0.042681   \n",
       "14541  0.004942 -0.000126  0.029034 -0.011315  0.000936 -0.009829 -0.005649   \n",
       "14548 -0.067621  0.055089 -0.035550 -0.016057  0.030357  0.086635 -0.116914   \n",
       "14550 -0.001730 -0.000357  0.013551 -0.001779 -0.008815  0.000180  0.003282   \n",
       "\n",
       "        node_98   node_99  my_label  \n",
       "myID                                 \n",
       "0      0.026843 -0.021281         0  \n",
       "2     -0.091207  0.132493         0  \n",
       "5     -0.040594 -0.096454         0  \n",
       "13     0.140799 -0.118054         0  \n",
       "14    -0.005635  0.040090         0  \n",
       "...         ...       ...       ...  \n",
       "14531 -0.013440 -0.017793         0  \n",
       "14532 -0.006727 -0.020374         0  \n",
       "14541  0.001292 -0.005044         0  \n",
       "14548 -0.061450 -0.064007         1  \n",
       "14550 -0.002532 -0.006610         0  \n",
       "\n",
       "[2536 rows x 103 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_col = label_name\n",
    "node_dataset[label_col] = node_dataset[label_col].astype('Int32')\n",
    "\n",
    "# TODO: decide whether or not to include network embedding features...\n",
    "num_node_feats = 100\n",
    "node_feat_cols = ['Tissue RNA - lung [NX]', 'Single Cell Type RNA - Mucus-secreting cells [NX]'] + [f'node_{i}' for i in range(num_node_feats)]\n",
    "\n",
    "# get subset of node features features + labels\n",
    "node_data = node_dataset[node_feat_cols + [label_col]]\n",
    "\n",
    "X = torch.Tensor(node_data[node_feat_cols].to_numpy())#.type(torch.float64)\n",
    "\n",
    "y = node_data[label_col].fillna(-1).astype('int') # fill NaN with -1 so that it can be converted to pytorch tensor\n",
    "y = torch.Tensor(y).type(torch.int64)\n",
    "\n",
    "# restrict to data with labels\n",
    "node_data_labeled = node_data[node_data[label_col].notna()]\n",
    "node_data_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_myIDs = node_data_labeled.index.to_numpy() # myIDs for nodes with labels for training/testing\n",
    "labels = node_data_labeled[label_col].to_numpy() # for stratification\n",
    "\n",
    "test_size = 0.2\n",
    "val_size = 0.1 * (1/(1-test_size))\n",
    "\n",
    "myIDs_train_val, myIDs_test = train_test_split(X_myIDs, test_size=test_size, shuffle=True, stratify=labels)\n",
    "\n",
    "labels_train_val = node_data_labeled.loc[myIDs_train_val][label_col].to_numpy()\n",
    "myIDs_train, myIDs_val = train_test_split(myIDs_train_val, test_size=val_size, shuffle=True, stratify=labels_train_val)\n",
    "\n",
    "# NOTE: train-val-test split is shuffled and stratified\n",
    "# TODO: look into any special consideration necessary for train-test splits on graph-based models\n",
    "\n",
    "# create masks\n",
    "n_nodes = len(node_data)\n",
    "train_mask = np.zeros(n_nodes, dtype=bool)\n",
    "train_mask[myIDs_train] = True\n",
    "train_mask = torch.Tensor(train_mask).type(torch.bool)\n",
    "\n",
    "val_mask = np.zeros(n_nodes, dtype=bool)\n",
    "val_mask[myIDs_val] = True\n",
    "val_mask = torch.Tensor(val_mask).type(torch.bool)\n",
    "\n",
    "test_mask = np.zeros(n_nodes, dtype=bool)\n",
    "test_mask[myIDs_test] = True\n",
    "test_mask = torch.Tensor(test_mask).type(torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(x=X, y=y, edge_index=edge_list)\n",
    "num_classes = 2\n",
    "num_features = X.shape[1]\n",
    "\n",
    "data.train_mask = train_mask\n",
    "data.val_mask = val_mask\n",
    "data.test_mask = test_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (convs): ModuleList(\n",
      "    (0): GCNConv(102, 16)\n",
      "    (1): GCNConv(16, 16)\n",
      "    (2): GCNConv(16, 16)\n",
      "    (3): GCNConv(16, 16)\n",
      "    (4): GCNConv(16, 16)\n",
      "    (5): GCNConv(16, 16)\n",
      "    (6): GCNConv(16, 16)\n",
      "    (7): GCNConv(16, 16)\n",
      "    (8): GCNConv(16, 16)\n",
      "    (9): GCNConv(16, 16)\n",
      "  )\n",
      "  (dense1): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (dense_out): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define GCN architecture\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, num_layers, dropout_rate=0):\n",
    "        super(GCN, self).__init__()\n",
    "        self.convs = []\n",
    "        self.convs.append(GCNConv(num_features, hidden_channels)) # first GCNConv layer\n",
    "\n",
    "        for _ in range(num_layers - 1): # middle layers\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "\n",
    "        self.convs = torch.nn.ModuleList(self.convs)\n",
    "\n",
    "        self.dense1 = torch.nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.dense_out = torch.nn.Linear(hidden_channels, num_classes)\n",
    "\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = x.relu()\n",
    "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "        x = self.dense1(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "        x = self.dense_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=16, num_layers=10, dropout_rate=0)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "# define Pytorch Lightning model\n",
    "class LitGCN(pl.LightningModule):\n",
    "    def __init__(self, model_name, **model_kwargs):\n",
    "        super().__init__()\n",
    "        # Saving hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.model = GCN(**model_kwargs)\n",
    "        self.loss_module = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        self.example_input_array = data\n",
    "\n",
    "    def forward(self, data, mode=\"train\"):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.model(x, edge_index)\n",
    "\n",
    "        # Only calculate the loss and acc on the nodes corresponding to the mask\n",
    "        if mode == \"train\":\n",
    "            mask = data.train_mask\n",
    "        elif mode == \"val\":\n",
    "            mask = data.val_mask\n",
    "        elif mode == \"test\":\n",
    "            mask = data.test_mask\n",
    "        else:\n",
    "            assert False, \"Unknown forward mode: %s\" % mode\n",
    "\n",
    "        #TODO: add other metrics like recall, precision, f1, etc...\n",
    "        loss = self.loss_module(x[mask], data.y[mask])\n",
    "        acc = (x[mask].argmax(dim=-1) == data.y[mask]).sum().float() / mask.sum()\n",
    "        return x, loss, acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters())#SGD(self.parameters(), lr=0.1, momentum=0.9, weight_decay=2e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, loss, acc = self.forward(batch, mode=\"train\")\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        logits, _, acc = self.forward(batch, mode=\"val\")\n",
    "        self.log(\"val_acc\", acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return logits\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        # NOTE: can't save non-standard GNN model like this\n",
    "        # TODO: look into how to save torch geometric models\n",
    "        # dummy_input = data\n",
    "        # model_filename = f'{self.model_name}_{str(self.global_step).zfill(5)}.onnx'\n",
    "        # torch.onnx.export(self, dummy_input, model_filename)\n",
    "        # wandb.save(model_filename)\n",
    "\n",
    "        flattened_logits = torch.flatten(torch.cat(validation_step_outputs))\n",
    "        self.logger.experiment.log({'val_logits': wandb.Histogram(flattened_logits.to('cpu')), \n",
    "                                    'global_step': self.global_step})\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, _, acc = self.forward(batch, mode=\"test\")\n",
    "        self.log(\"test_acc\", acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "    # def test_epoch_end(self, test_step_outputs):\n",
    "    #     # save model as onnx format\n",
    "    #     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "notebook_name = 'modeling_gnn.ipynb'\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = notebook_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mawni00\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/awni00/Project%20X/runs/2icis8k6\" target=\"_blank\">my_gcn_test3</a></strong> to <a href=\"https://wandb.ai/awni00/Project%20X\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name        | Type             | Params | In sizes                     | Out sizes \n",
      "---------------------------------------------------------------------------------------------\n",
      "0 | model       | GCN              | 2.2 K  | [[14552, 102], [2, 4214097]] | [14552, 2]\n",
      "1 | loss_module | CrossEntropyLoss | 0      | [[1774, 2], [1774]]          | ?         \n",
      "---------------------------------------------------------------------------------------------\n",
      "2.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.2 K     Total params\n",
      "0.009     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\awnya\\miniconda3\\envs\\projectx\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\awnya\\miniconda3\\envs\\projectx\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\awnya\\miniconda3\\envs\\projectx\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:326: UserWarning: The number of training samples (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 2/2 [00:00<00:00,  6.29it/s, loss=0.672, v_num=s8k6, train_loss_step=0.663, train_acc_step=0.639, val_acc_step=0.638, val_acc_epoch=0.638, train_loss_epoch=0.664, train_acc_epoch=0.639]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "import wandb\n",
    "import torch_geometric.loader\n",
    "\n",
    "model_name = 'my_gcn_test3'\n",
    "\n",
    "# logger = TensorBoardLogger(\"tb_logs\", name=model_name)#, log_graph=True)\n",
    "logger = WandbLogger(name=model_name, project=\"Project X\", log_model=\"all\")#, version=...)\n",
    "\n",
    "\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "# AVAIL_GPUS = 0 # use when running out VRAM\n",
    "\n",
    "model = LitGCN(model_name, hidden_channels=16, num_layers=2)#hidden_channels=64, num_layers=10, dropout_rate=0)\n",
    "\n",
    "data_loader = torch_geometric.loader.DataLoader([data])#, batch_size=1, num_workers=2)\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "        callbacks=[ModelCheckpoint(save_weights_only=False, mode=\"max\", monitor=\"val_acc\")],\n",
    "        gpus=AVAIL_GPUS,\n",
    "        max_epochs=50,\n",
    "        logger=logger,\n",
    "        # progress_bar_refresh_rate=0,\n",
    "    )  # 0 because epoch size is 1\n",
    "\n",
    "trainer.fit(model, data_loader, data_loader)\n",
    "model = LitGCN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.63      0.64       887\n",
      "    positive       0.64      0.65      0.64       887\n",
      "\n",
      "    accuracy                           0.64      1774\n",
      "   macro avg       0.64      0.64      0.64      1774\n",
      "weighted avg       0.64      0.64      0.64      1774\n",
      "\n",
      "\n",
      "testing metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.63      0.65      0.64       254\n",
      "    positive       0.64      0.62      0.63       254\n",
      "\n",
      "    accuracy                           0.64       508\n",
      "   macro avg       0.64      0.64      0.64       508\n",
      "weighted avg       0.64      0.64      0.64       508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "logits, _, _ = model.forward(data.to(device='cpu'))\n",
    "\n",
    "preds_train = logits[data.train_mask].argmax(dim=-1)\n",
    "preds_test = logits[data.test_mask].argmax(dim=-1)\n",
    "\n",
    "y_train = data.y[data.train_mask]\n",
    "y_test = data.y[data.test_mask]\n",
    "\n",
    "train_report = classification_report(y_train, preds_train, labels=[0,1], target_names=['negative', 'positive'])\n",
    "test_report = classification_report(y_test, preds_test, labels=[0,1], target_names=['negative', 'positive'])\n",
    "\n",
    "print('training metrics')\n",
    "print(train_report)\n",
    "print()\n",
    "print('testing metrics')\n",
    "print(test_report)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c536624f42ad28a25b71910f95eefc8498c618baf8c306d00b21bed7daec08b1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('projectx': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
