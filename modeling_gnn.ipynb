{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load edge list\n",
    "edge_list_path = 'data/edge_list.npy'\n",
    "edge_list = torch.Tensor(np.load(edge_list_path).T).type(torch.int64) # read in format expected by pytorch geometric [2, n_edges]\n",
    "\n",
    "# load protein-ID dictionary (need new ID system starting at index 0 for pytorch geometric)\n",
    "protein_id_dict = np.load('data/protein_ids_dict.npy', allow_pickle=True).item() # maps my custom ID system to Ensembl IDs\n",
    "protein_id_dict_inv = {Ensembl: id_ for id_, Ensembl in protein_id_dict.items()} # maps Ensembl IDs to my custom ID system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/HPAnode_PPInetwork_labels_tempv2.csv' #FIXME: currently dataset has no negative labels\n",
    "node_dataset = pd.read_csv(data_path, index_col=0)\n",
    "\n",
    "# map dataset\n",
    "myID = node_dataset.index.map(protein_id_dict_inv).rename('myID')\n",
    "node_dataset.insert(loc=0, column='myID', value=myID)\n",
    "node_dataset = node_dataset.reset_index().set_index('myID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure dataset with myID is of correct order and format\n",
    "node_dataset.sort_index(inplace=True) # should already be sorted, but just in case\n",
    "assert((node_dataset.index.to_numpy() == np.arange(len(node_dataset))).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of labels\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    521\n",
       "0    135\n",
       "Name: my_label, dtype: Int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FIXME: this is a temporary label. need to look into getting positive and negative labels.\n",
    "# label: 1 if NIH_pos, 0 if not NIH_Cancer, NaN otherwise\n",
    "label_name = 'my_label'\n",
    "def my_labeler(NIH_pos, NIH_cancer):\n",
    "    if NIH_pos and not NIH_cancer:\n",
    "        raise ValueError('Data inconsistent. Found row with NIH label both positive and negative')\n",
    "    if NIH_pos:\n",
    "        return 1\n",
    "    elif not NIH_cancer:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "my_labels = pd.array([my_labeler(row.NIH_pos, row.NIH_Cancer) for id_, row in node_dataset.iterrows()], dtype='Int32')\n",
    "\n",
    "node_dataset[label_name] = my_labels\n",
    "\n",
    "print('Distribution of labels')\n",
    "node_dataset[label_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tissue RNA - lung [NX]</th>\n",
       "      <th>Single Cell Type RNA - Mucus-secreting cells [NX]</th>\n",
       "      <th>node_0</th>\n",
       "      <th>node_1</th>\n",
       "      <th>node_2</th>\n",
       "      <th>node_3</th>\n",
       "      <th>node_4</th>\n",
       "      <th>node_5</th>\n",
       "      <th>node_6</th>\n",
       "      <th>node_7</th>\n",
       "      <th>...</th>\n",
       "      <th>node_91</th>\n",
       "      <th>node_92</th>\n",
       "      <th>node_93</th>\n",
       "      <th>node_94</th>\n",
       "      <th>node_95</th>\n",
       "      <th>node_96</th>\n",
       "      <th>node_97</th>\n",
       "      <th>node_98</th>\n",
       "      <th>node_99</th>\n",
       "      <th>my_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>myID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.996506</td>\n",
       "      <td>0.038446</td>\n",
       "      <td>1.537459</td>\n",
       "      <td>0.429451</td>\n",
       "      <td>-0.302780</td>\n",
       "      <td>0.258347</td>\n",
       "      <td>-0.950399</td>\n",
       "      <td>1.365928</td>\n",
       "      <td>-0.064216</td>\n",
       "      <td>0.228324</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.115296</td>\n",
       "      <td>-0.234800</td>\n",
       "      <td>0.049897</td>\n",
       "      <td>0.382442</td>\n",
       "      <td>0.168788</td>\n",
       "      <td>-0.033863</td>\n",
       "      <td>0.065672</td>\n",
       "      <td>0.088566</td>\n",
       "      <td>0.006333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.150101</td>\n",
       "      <td>-0.069827</td>\n",
       "      <td>2.622879</td>\n",
       "      <td>0.092524</td>\n",
       "      <td>1.558535</td>\n",
       "      <td>-1.148822</td>\n",
       "      <td>0.606971</td>\n",
       "      <td>0.573626</td>\n",
       "      <td>0.106728</td>\n",
       "      <td>-0.357630</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.327284</td>\n",
       "      <td>-0.087676</td>\n",
       "      <td>0.254183</td>\n",
       "      <td>-0.066311</td>\n",
       "      <td>-0.014220</td>\n",
       "      <td>-0.059492</td>\n",
       "      <td>0.095315</td>\n",
       "      <td>0.159288</td>\n",
       "      <td>-0.186821</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.046881</td>\n",
       "      <td>-0.103977</td>\n",
       "      <td>1.976907</td>\n",
       "      <td>-1.347319</td>\n",
       "      <td>1.559400</td>\n",
       "      <td>-0.076801</td>\n",
       "      <td>0.088430</td>\n",
       "      <td>0.544722</td>\n",
       "      <td>0.046640</td>\n",
       "      <td>-0.046761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037351</td>\n",
       "      <td>-0.026250</td>\n",
       "      <td>-0.042694</td>\n",
       "      <td>0.015399</td>\n",
       "      <td>-0.069292</td>\n",
       "      <td>-0.039175</td>\n",
       "      <td>-0.089943</td>\n",
       "      <td>0.059104</td>\n",
       "      <td>-0.008584</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>-0.211169</td>\n",
       "      <td>-0.105079</td>\n",
       "      <td>1.302629</td>\n",
       "      <td>0.359702</td>\n",
       "      <td>0.487322</td>\n",
       "      <td>-0.169744</td>\n",
       "      <td>-0.610400</td>\n",
       "      <td>0.668143</td>\n",
       "      <td>-0.180892</td>\n",
       "      <td>-0.021217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075845</td>\n",
       "      <td>-0.315101</td>\n",
       "      <td>0.056129</td>\n",
       "      <td>0.535637</td>\n",
       "      <td>0.085542</td>\n",
       "      <td>-0.076495</td>\n",
       "      <td>-0.089464</td>\n",
       "      <td>0.086111</td>\n",
       "      <td>0.017891</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>-0.655015</td>\n",
       "      <td>-0.105079</td>\n",
       "      <td>1.480835</td>\n",
       "      <td>0.846645</td>\n",
       "      <td>0.805566</td>\n",
       "      <td>0.485862</td>\n",
       "      <td>-0.194441</td>\n",
       "      <td>0.923558</td>\n",
       "      <td>1.438937</td>\n",
       "      <td>-0.492804</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031383</td>\n",
       "      <td>-0.038140</td>\n",
       "      <td>-0.029342</td>\n",
       "      <td>-0.007463</td>\n",
       "      <td>-0.029591</td>\n",
       "      <td>-0.034695</td>\n",
       "      <td>-0.038255</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14547</th>\n",
       "      <td>-0.634371</td>\n",
       "      <td>-0.105079</td>\n",
       "      <td>0.700987</td>\n",
       "      <td>-0.087903</td>\n",
       "      <td>-0.319591</td>\n",
       "      <td>0.006010</td>\n",
       "      <td>-0.224073</td>\n",
       "      <td>-0.241506</td>\n",
       "      <td>-0.419347</td>\n",
       "      <td>0.136663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038222</td>\n",
       "      <td>-0.073042</td>\n",
       "      <td>0.162214</td>\n",
       "      <td>-0.065836</td>\n",
       "      <td>0.078069</td>\n",
       "      <td>0.020572</td>\n",
       "      <td>-0.251188</td>\n",
       "      <td>-0.065744</td>\n",
       "      <td>0.097520</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14548</th>\n",
       "      <td>-0.603405</td>\n",
       "      <td>0.835389</td>\n",
       "      <td>2.009425</td>\n",
       "      <td>1.142514</td>\n",
       "      <td>0.587500</td>\n",
       "      <td>-1.108236</td>\n",
       "      <td>-0.523388</td>\n",
       "      <td>0.751758</td>\n",
       "      <td>0.338092</td>\n",
       "      <td>0.156393</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067621</td>\n",
       "      <td>0.055089</td>\n",
       "      <td>-0.035550</td>\n",
       "      <td>-0.016057</td>\n",
       "      <td>0.030357</td>\n",
       "      <td>0.086635</td>\n",
       "      <td>-0.116914</td>\n",
       "      <td>-0.061450</td>\n",
       "      <td>-0.064007</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14549</th>\n",
       "      <td>-0.618888</td>\n",
       "      <td>-0.105079</td>\n",
       "      <td>1.292300</td>\n",
       "      <td>0.347360</td>\n",
       "      <td>-0.751088</td>\n",
       "      <td>-0.469017</td>\n",
       "      <td>0.258115</td>\n",
       "      <td>0.182103</td>\n",
       "      <td>0.151207</td>\n",
       "      <td>0.596934</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028228</td>\n",
       "      <td>-0.029454</td>\n",
       "      <td>-0.024224</td>\n",
       "      <td>-0.004768</td>\n",
       "      <td>-0.009325</td>\n",
       "      <td>-0.014907</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.002426</td>\n",
       "      <td>-0.016972</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14550</th>\n",
       "      <td>-0.422770</td>\n",
       "      <td>-0.092489</td>\n",
       "      <td>0.942309</td>\n",
       "      <td>-0.071680</td>\n",
       "      <td>-0.648776</td>\n",
       "      <td>0.007953</td>\n",
       "      <td>-0.479971</td>\n",
       "      <td>0.004475</td>\n",
       "      <td>0.273699</td>\n",
       "      <td>0.507728</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001730</td>\n",
       "      <td>-0.000357</td>\n",
       "      <td>0.013551</td>\n",
       "      <td>-0.001779</td>\n",
       "      <td>-0.008815</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.003282</td>\n",
       "      <td>-0.002532</td>\n",
       "      <td>-0.006610</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14551</th>\n",
       "      <td>-0.644693</td>\n",
       "      <td>-0.105079</td>\n",
       "      <td>0.696272</td>\n",
       "      <td>-0.090576</td>\n",
       "      <td>-0.319852</td>\n",
       "      <td>0.011046</td>\n",
       "      <td>-0.219712</td>\n",
       "      <td>-0.229885</td>\n",
       "      <td>-0.413649</td>\n",
       "      <td>0.130573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059803</td>\n",
       "      <td>-0.298359</td>\n",
       "      <td>0.130348</td>\n",
       "      <td>0.579840</td>\n",
       "      <td>0.220878</td>\n",
       "      <td>0.017755</td>\n",
       "      <td>0.047654</td>\n",
       "      <td>0.056070</td>\n",
       "      <td>0.055850</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>656 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Tissue RNA - lung [NX]  \\\n",
       "myID                            \n",
       "23                   0.996506   \n",
       "80                   0.150101   \n",
       "116                  0.046881   \n",
       "146                 -0.211169   \n",
       "149                 -0.655015   \n",
       "...                       ...   \n",
       "14547               -0.634371   \n",
       "14548               -0.603405   \n",
       "14549               -0.618888   \n",
       "14550               -0.422770   \n",
       "14551               -0.644693   \n",
       "\n",
       "       Single Cell Type RNA - Mucus-secreting cells [NX]    node_0    node_1  \\\n",
       "myID                                                                           \n",
       "23                                              0.038446  1.537459  0.429451   \n",
       "80                                             -0.069827  2.622879  0.092524   \n",
       "116                                            -0.103977  1.976907 -1.347319   \n",
       "146                                            -0.105079  1.302629  0.359702   \n",
       "149                                            -0.105079  1.480835  0.846645   \n",
       "...                                                  ...       ...       ...   \n",
       "14547                                          -0.105079  0.700987 -0.087903   \n",
       "14548                                           0.835389  2.009425  1.142514   \n",
       "14549                                          -0.105079  1.292300  0.347360   \n",
       "14550                                          -0.092489  0.942309 -0.071680   \n",
       "14551                                          -0.105079  0.696272 -0.090576   \n",
       "\n",
       "         node_2    node_3    node_4    node_5    node_6    node_7  ...  \\\n",
       "myID                                                               ...   \n",
       "23    -0.302780  0.258347 -0.950399  1.365928 -0.064216  0.228324  ...   \n",
       "80     1.558535 -1.148822  0.606971  0.573626  0.106728 -0.357630  ...   \n",
       "116    1.559400 -0.076801  0.088430  0.544722  0.046640 -0.046761  ...   \n",
       "146    0.487322 -0.169744 -0.610400  0.668143 -0.180892 -0.021217  ...   \n",
       "149    0.805566  0.485862 -0.194441  0.923558  1.438937 -0.492804  ...   \n",
       "...         ...       ...       ...       ...       ...       ...  ...   \n",
       "14547 -0.319591  0.006010 -0.224073 -0.241506 -0.419347  0.136663  ...   \n",
       "14548  0.587500 -1.108236 -0.523388  0.751758  0.338092  0.156393  ...   \n",
       "14549 -0.751088 -0.469017  0.258115  0.182103  0.151207  0.596934  ...   \n",
       "14550 -0.648776  0.007953 -0.479971  0.004475  0.273699  0.507728  ...   \n",
       "14551 -0.319852  0.011046 -0.219712 -0.229885 -0.413649  0.130573  ...   \n",
       "\n",
       "        node_91   node_92   node_93   node_94   node_95   node_96   node_97  \\\n",
       "myID                                                                          \n",
       "23    -0.115296 -0.234800  0.049897  0.382442  0.168788 -0.033863  0.065672   \n",
       "80    -0.327284 -0.087676  0.254183 -0.066311 -0.014220 -0.059492  0.095315   \n",
       "116    0.037351 -0.026250 -0.042694  0.015399 -0.069292 -0.039175 -0.089943   \n",
       "146    0.075845 -0.315101  0.056129  0.535637  0.085542 -0.076495 -0.089464   \n",
       "149   -0.031383 -0.038140 -0.029342 -0.007463 -0.029591 -0.034695 -0.038255   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "14547 -0.038222 -0.073042  0.162214 -0.065836  0.078069  0.020572 -0.251188   \n",
       "14548 -0.067621  0.055089 -0.035550 -0.016057  0.030357  0.086635 -0.116914   \n",
       "14549 -0.028228 -0.029454 -0.024224 -0.004768 -0.009325 -0.014907 -0.000052   \n",
       "14550 -0.001730 -0.000357  0.013551 -0.001779 -0.008815  0.000180  0.003282   \n",
       "14551  0.059803 -0.298359  0.130348  0.579840  0.220878  0.017755  0.047654   \n",
       "\n",
       "        node_98   node_99  my_label  \n",
       "myID                                 \n",
       "23     0.088566  0.006333         1  \n",
       "80     0.159288 -0.186821         1  \n",
       "116    0.059104 -0.008584         1  \n",
       "146    0.086111  0.017891         1  \n",
       "149    0.000011  0.000772         1  \n",
       "...         ...       ...       ...  \n",
       "14547 -0.065744  0.097520         0  \n",
       "14548 -0.061450 -0.064007         0  \n",
       "14549 -0.002426 -0.016972         0  \n",
       "14550 -0.002532 -0.006610         0  \n",
       "14551  0.056070  0.055850         0  \n",
       "\n",
       "[656 rows x 103 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_col = label_name\n",
    "node_dataset[label_col] = node_dataset[label_col].astype('Int32')\n",
    "\n",
    "# TODO: decide whether or not to include network embedding features...\n",
    "num_node_feats = 100\n",
    "node_feat_cols = ['Tissue RNA - lung [NX]', 'Single Cell Type RNA - Mucus-secreting cells [NX]'] + [f'node_{i}' for i in range(num_node_feats)]\n",
    "\n",
    "# get subset of node features features + labels\n",
    "node_data = node_dataset[node_feat_cols + [label_col]]\n",
    "\n",
    "X = torch.Tensor(node_data[node_feat_cols].to_numpy())#.type(torch.float64)\n",
    "\n",
    "y = node_data[label_col].fillna(-1).astype('int') # fill NaN with -1 so that it can be converted to pytorch tensor\n",
    "y = torch.Tensor(y).type(torch.int64)\n",
    "\n",
    "# restrict to data with labels\n",
    "node_data_labeled = node_data[node_data[label_col].notna()]\n",
    "node_data_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_myIDs = node_data_labeled.index.to_numpy() # myIDs for nodes with labels for training/testing\n",
    "labels = node_data_labeled[label_col].to_numpy() # for stratification\n",
    "\n",
    "test_size = 0.2\n",
    "val_size = 0.1 * (1/(1-test_size))\n",
    "\n",
    "myIDs_train_val, myIDs_test = train_test_split(X_myIDs, test_size=test_size, shuffle=True, stratify=labels)\n",
    "\n",
    "labels_train_val = node_data_labeled.loc[myIDs_train_val][label_col].to_numpy()\n",
    "myIDs_train, myIDs_val = train_test_split(myIDs_train_val, test_size=val_size, shuffle=True, stratify=labels_train_val)\n",
    "\n",
    "# NOTE: train-val-test split is shuffled and stratified\n",
    "# TODO: look into any special consideration necessary for train-test splits on graph-based models\n",
    "\n",
    "# create masks\n",
    "n_nodes = len(node_data)\n",
    "train_mask = np.zeros(n_nodes, dtype=bool)\n",
    "train_mask[myIDs_train] = True\n",
    "train_mask = torch.Tensor(train_mask).type(torch.bool)\n",
    "\n",
    "val_mask = np.zeros(n_nodes, dtype=bool)\n",
    "val_mask[myIDs_val] = True\n",
    "val_mask = torch.Tensor(val_mask).type(torch.bool)\n",
    "\n",
    "test_mask = np.zeros(n_nodes, dtype=bool)\n",
    "test_mask[myIDs_test] = True\n",
    "test_mask = torch.Tensor(test_mask).type(torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(x=X, y=y, edge_index=edge_list)\n",
    "num_classes = 2\n",
    "num_features = X.shape[1]\n",
    "\n",
    "data.train_mask = train_mask\n",
    "data.val_mask = val_mask\n",
    "data.test_mask = test_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (convs): ModuleList(\n",
      "    (0): GCNConv(102, 16)\n",
      "    (1): GCNConv(16, 16)\n",
      "    (2): GCNConv(16, 16)\n",
      "    (3): GCNConv(16, 16)\n",
      "    (4): GCNConv(16, 16)\n",
      "    (5): GCNConv(16, 16)\n",
      "    (6): GCNConv(16, 16)\n",
      "    (7): GCNConv(16, 16)\n",
      "    (8): GCNConv(16, 16)\n",
      "    (9): GCNConv(16, 16)\n",
      "  )\n",
      "  (dense1): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (dense_out): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# # define GCN architecture\n",
    "# class GCN(torch.nn.Module):\n",
    "#     def __init__(self, hidden_channels, num_layers, dropout_rate=0):\n",
    "#         super(GCN, self).__init__()\n",
    "#         self.convs = []\n",
    "#         self.convs.append(GCNConv(num_features, hidden_channels)) # first GCNConv layer\n",
    "\n",
    "#         for _ in range(num_layers - 2): # middle layers\n",
    "#             self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "\n",
    "#         self.convs.append(GCNConv(hidden_channels, num_classes)) # last GCNConv layer\n",
    "#         self.convs = torch.nn.ModuleList(self.convs)\n",
    "\n",
    "#         self.dropout_rate = dropout_rate\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         for conv in self.convs:\n",
    "#             x = conv(x, edge_index)\n",
    "#             x = x.relu()\n",
    "#             x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "#         return x\n",
    "\n",
    "# define GCN architecture\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, num_layers, dropout_rate=0):\n",
    "        super(GCN, self).__init__()\n",
    "        self.convs = []\n",
    "        self.convs.append(GCNConv(num_features, hidden_channels)) # first GCNConv layer\n",
    "\n",
    "        for _ in range(num_layers - 1): # middle layers\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "\n",
    "        # self.convs.append(GCNConv(hidden_channels, num_classes)) # last GCNConv layer\n",
    "        self.convs = torch.nn.ModuleList(self.convs)\n",
    "\n",
    "        self.dense1 = torch.nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.dense_out = torch.nn.Linear(hidden_channels, num_classes)\n",
    "\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = x.relu()\n",
    "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "        x = self.dense1(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "        x = self.dense_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=16, num_layers=10, dropout_rate=0)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "# define Pytorch Lightning model\n",
    "class LitGCN(pl.LightningModule):\n",
    "    def __init__(self, model_name, **model_kwargs):\n",
    "        super().__init__()\n",
    "        # Saving hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.model = GCN(**model_kwargs)\n",
    "        self.loss_module = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        self.example_input_array = data\n",
    "\n",
    "    def forward(self, data, mode=\"train\"):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.model(x, edge_index)\n",
    "\n",
    "        # Only calculate the loss and acc on the nodes corresponding to the mask\n",
    "        if mode == \"train\":\n",
    "            mask = data.train_mask\n",
    "        elif mode == \"val\":\n",
    "            mask = data.val_mask\n",
    "        elif mode == \"test\":\n",
    "            mask = data.test_mask\n",
    "        else:\n",
    "            assert False, \"Unknown forward mode: %s\" % mode\n",
    "\n",
    "        #TODO: add other metrics like recall, precision, f1, etc...\n",
    "        loss = self.loss_module(x[mask], data.y[mask])\n",
    "        acc = (x[mask].argmax(dim=-1) == data.y[mask]).sum().float() / mask.sum()\n",
    "        return x, loss, acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters())#SGD(self.parameters(), lr=0.1, momentum=0.9, weight_decay=2e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, loss, acc = self.forward(batch, mode=\"train\")\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        logits, _, acc = self.forward(batch, mode=\"val\")\n",
    "        self.log(\"val_acc\", acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return logits\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        # NOTE: can't save non-standard GNN model like this\n",
    "        # TODO: look into how to save torch geometric models\n",
    "        # dummy_input = data\n",
    "        # model_filename = f'{self.model_name}_{str(self.global_step).zfill(5)}.onnx'\n",
    "        # torch.onnx.export(self, dummy_input, model_filename)\n",
    "        # wandb.save(model_filename)\n",
    "\n",
    "        flattened_logits = torch.flatten(torch.cat(validation_step_outputs))\n",
    "        self.logger.experiment.log({'val_logits': wandb.Histogram(flattened_logits.to('cpu')), \n",
    "                                    'global_step': self.global_step})\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, _, acc = self.forward(batch, mode=\"test\")\n",
    "        self.log(\"test_acc\", acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "    # def test_epoch_end(self, test_step_outputs):\n",
    "    #     # save model as onnx format\n",
    "    #     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "notebook_name = 'modeling_gnn.ipynb'\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = notebook_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mawni00\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/awni00/Project%20X/runs/1oha2kjm\" target=\"_blank\">my_gcn_test2</a></strong> to <a href=\"https://wandb.ai/awni00/Project%20X\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name        | Type             | Params | In sizes                     | Out sizes \n",
      "---------------------------------------------------------------------------------------------\n",
      "0 | model       | GCN              | 2.2 K  | [[14552, 102], [2, 4214097]] | [14552, 2]\n",
      "1 | loss_module | CrossEntropyLoss | 0      | [[458, 2], [458]]            | ?         \n",
      "---------------------------------------------------------------------------------------------\n",
      "2.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.2 K     Total params\n",
      "0.009     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\awnya\\miniconda3\\envs\\projectx\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\awnya\\miniconda3\\envs\\projectx\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\awnya\\miniconda3\\envs\\projectx\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:326: UserWarning: The number of training samples (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 2/2 [00:00<00:00,  5.34it/s, loss=0.568, v_num=2kjm, train_loss_step=0.508, train_acc_step=0.795, val_acc_step=0.788, val_acc_epoch=0.788, train_loss_epoch=0.515, train_acc_epoch=0.795]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "import wandb\n",
    "import torch_geometric.loader\n",
    "\n",
    "model_name = 'my_gcn_test2'\n",
    "\n",
    "# logger = TensorBoardLogger(\"tb_logs\", name=model_name)#, log_graph=True)\n",
    "logger = WandbLogger(name=model_name, project=\"Project X\", log_model=\"all\")#, version=...)\n",
    "\n",
    "\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "# AVAIL_GPUS = 0 # use when running out VRAM\n",
    "\n",
    "model = LitGCN(model_name, hidden_channels=16, num_layers=2)#hidden_channels=64, num_layers=10, dropout_rate=0)\n",
    "\n",
    "data_loader = torch_geometric.loader.DataLoader([data])#, batch_size=1, num_workers=2)\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "        callbacks=[ModelCheckpoint(save_weights_only=False, mode=\"max\", monitor=\"val_acc\")],\n",
    "        gpus=AVAIL_GPUS,\n",
    "        max_epochs=50,\n",
    "        logger=logger,\n",
    "        # progress_bar_refresh_rate=0,\n",
    "    )  # 0 because epoch size is 1\n",
    "\n",
    "trainer.fit(model, data_loader, data_loader)\n",
    "model = LitGCN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.43      0.54        94\n",
      "    positive       0.87      0.96      0.91       364\n",
      "\n",
      "    accuracy                           0.85       458\n",
      "   macro avg       0.80      0.69      0.72       458\n",
      "weighted avg       0.84      0.85      0.83       458\n",
      "\n",
      "\n",
      "testing metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.56      0.67        27\n",
      "    positive       0.89      0.97      0.93       105\n",
      "\n",
      "    accuracy                           0.89       132\n",
      "   macro avg       0.86      0.76      0.80       132\n",
      "weighted avg       0.88      0.89      0.88       132\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "logits, _, _ = model.forward(data.to(device='cpu'))\n",
    "\n",
    "preds_train = logits[data.train_mask].argmax(dim=-1)\n",
    "preds_test = logits[data.test_mask].argmax(dim=-1)\n",
    "\n",
    "y_train = data.y[data.train_mask]\n",
    "y_test = data.y[data.test_mask]\n",
    "\n",
    "train_report = classification_report(y_train, preds_train, labels=[0,1], target_names=['negative', 'positive'])\n",
    "test_report = classification_report(y_test, preds_test, labels=[0,1], target_names=['negative', 'positive'])\n",
    "\n",
    "print('training metrics')\n",
    "print(train_report)\n",
    "print()\n",
    "print('testing metrics')\n",
    "print(test_report)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c536624f42ad28a25b71910f95eefc8498c618baf8c306d00b21bed7daec08b1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('projectx': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
